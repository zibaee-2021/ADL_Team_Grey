{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/beanburger/Documents/_ML_UCL/_ML_MODULES/T2_MODULES/ASSIGNMENTS2/ADL_GROUP_CW/ADL_Team_Grey/augmented_data'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOUNT YOUR GOOGLE DRIVE\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# CHANGE CURRENT DIRECTORY TO FOLDER YOU HAvE THE CODE IN YOUR GOOGLE DRIVE\n",
    "import os\n",
    "# so that code from local project directory works here in Colab\n",
    "os.chdir('/content/drive/MyDrive/TEAM_GREY/src')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: filelock in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (4.10.0)\r\n",
      "Requirement already satisfied: sympy in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: torchvision in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (0.17.1)\r\n",
      "Requirement already satisfied: numpy in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: torch in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torchvision) (2.2.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torchvision) (10.2.0)\r\n",
      "Requirement already satisfied: filelock in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (4.10.0)\r\n",
      "Requirement already satisfied: sympy in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from torch->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from jinja2->torch->torchvision) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from sympy->torch->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-image in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (0.22.0)\r\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (1.12.0)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (3.2.1)\r\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (10.2.0)\r\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (2.34.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (2024.2.12)\r\n",
      "Requirement already satisfied: packaging>=21 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (24.0)\r\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages (from scikit-image) (0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-image\n",
    "# conda install -c conda-forge scikit-image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'v2' from 'torchvision.transforms' (/Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torchvision/transforms/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save_image\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mOxfordPetDataset\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mOxPetData\u001B[39;00m  \u001B[38;5;66;03m# Steve's `OxfordPetDataset` class from main file put into own file. I added in transform functionality to this class.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mconfig\u001B[39;00m  \u001B[38;5;66;03m# Contains Steve's `params` from the main file\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/_ML_UCL/_ML_MODULES/T2_MODULES/ASSIGNMENTS2/ADL_GROUP_CW/ADL_Team_Grey/augmented_data/OxfordPetDataset.py:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2 \u001B[38;5;28;01mas\u001B[39;00m v2T\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset, random_split\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'v2' from 'torchvision.transforms' (/Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torchvision/transforms/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import OxfordPetDataset as OxPetData  # Steve's `OxfordPetDataset` class from main file put into own file. I added in transform functionality to this class.\n",
    "import config  # Contains Steve's `params` from the main file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'v2' from 'torchvision.transforms' (/Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torchvision/transforms/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save_image\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mOxfordPetDataset\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mOxPetData\u001B[39;00m  \u001B[38;5;66;03m# Steve's `OxfordPetDataset` class from main file put into own file. I added in transform functionality to this class.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/_ML_UCL/_ML_MODULES/T2_MODULES/ASSIGNMENTS2/ADL_GROUP_CW/ADL_Team_Grey/augmented_data/OxfordPetDataset.py:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2 \u001B[38;5;28;01mas\u001B[39;00m v2T\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset, random_split\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'v2' from 'torchvision.transforms' (/Users/beanburger/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torchvision/transforms/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import OxfordPetDataset as OxPetData  # Steve's `OxfordPetDataset` class from main file put into own file. I added in transform functionality to this class.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### TRANSFORM WITHOUT AUGMENTATIONS (CALC MEAN & STD FOR NORMALISATION):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OxfordPetDataset.__init__() got an unexpected keyword argument 'aug_transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 9\u001B[0m\n\u001B[1;32m      2\u001B[0m transforms \u001B[38;5;241m=\u001B[39m v2T\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      3\u001B[0m     v2T\u001B[38;5;241m.\u001B[39mResize(size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m), antialias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# T.ToDtype(torch.float32, scale=True),  # Normalize expects float input\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     ])\n\u001B[1;32m      8\u001B[0m script_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../src/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 9\u001B[0m ox_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mOxPetData\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOxfordPetDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscript_dir\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mOxford-IIIT-Pet/images\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mlabel_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscript_dir\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mOxford-IIIT-Pet/annotations/trimaps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maug_transform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m ox_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mox_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean_std\u001B[39m(loader):\n",
      "\u001B[0;31mTypeError\u001B[0m: OxfordPetDataset.__init__() got an unexpected keyword argument 'aug_transform'"
     ]
    }
   ],
   "source": [
    "# expecting to receive a tensor to transform\n",
    "transforms = T.Compose([\n",
    "    T.Resize(size=(224, 224), antialias=True),\n",
    "    # T.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "script_dir = '../src/'\n",
    "ox_dataset = OxPetData.OxfordPetDataset(image_dir=script_dir+'Oxford-IIIT-Pet/images',\n",
    "                                        label_dir=script_dir+'Oxford-IIIT-Pet/annotations/trimaps',\n",
    "                                        parameters=config.params, aug_transform=transforms)\n",
    "\n",
    "ox_loader = DataLoader(dataset=ox_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "def mean_std(loader):\n",
    "    channels_sum, channels_sqd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_sqd_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean_ = channels_sum/num_batches\n",
    "    std_ = (channels_sqd_sum / num_batches - mean_ ** 2) ** 0.5\n",
    "    return mean_, std_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4811, 0.4492, 0.3958])\n",
      "tensor([0.2645, 0.2596, 0.2681])\n"
     ]
    }
   ],
   "source": [
    "mean, std = mean_std(ox_loader)\n",
    "print(mean)\n",
    "print(std)\n",
    "# a batch of 64 this gives:\n",
    "# tensor([0.4812, 0.4493, 0.3957])\n",
    "# tensor([0.2645, 0.2597, 0.2682])\n",
    "\n",
    "# a batch of 128 this gives:\n",
    "# tensor([0.4811, 0.4491, 0.3957])\n",
    "# tensor([0.2643, 0.2594, 0.2679])\n",
    "\n",
    "# a batch of 256 this gives:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### TRANSFORM WITH AUGMENTATIONS (CALC MEAN & STD FOR NORMALISATION):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m aug_transform \u001B[38;5;241m=\u001B[39m \u001B[43mT\u001B[49m\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      2\u001B[0m                      \u001B[38;5;66;03m# T.ToPILImage(),\u001B[39;00m\n\u001B[1;32m      3\u001B[0m                      T\u001B[38;5;241m.\u001B[39mColorJitter(brightness\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[1;32m      4\u001B[0m                      T\u001B[38;5;241m.\u001B[39mRandomHorizontalFlip(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[1;32m      5\u001B[0m                      T\u001B[38;5;241m.\u001B[39mRandomVerticalFlip(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m),\n\u001B[1;32m      6\u001B[0m                      T\u001B[38;5;241m.\u001B[39mRandomRotation(degrees\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m90\u001B[39m),\n\u001B[1;32m      7\u001B[0m                      T\u001B[38;5;241m.\u001B[39mRandomGrayscale(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m),\n\u001B[1;32m      8\u001B[0m                      T\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[1;32m      9\u001B[0m                      \u001B[38;5;66;03m# T.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\u001B[39;00m\n\u001B[1;32m     10\u001B[0m                      ])\n\u001B[1;32m     12\u001B[0m script_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../src/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     13\u001B[0m aug_ox_dataset \u001B[38;5;241m=\u001B[39m OxPetData\u001B[38;5;241m.\u001B[39mOxfordPetDataset(image_dir\u001B[38;5;241m=\u001B[39mscript_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOxford-IIIT-Pet/images\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     14\u001B[0m                                         label_dir\u001B[38;5;241m=\u001B[39mscript_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOxford-IIIT-Pet/annotations/trimaps\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     15\u001B[0m                                         parameters\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mparams, aug_transform\u001B[38;5;241m=\u001B[39maug_transform)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# expecting to receive a tensor to transform\n",
    "aug_transform = T.Compose([\n",
    "                     # T.ToPILImage(),\n",
    "                     T.ColorJitter(brightness=0.5),\n",
    "                     T.RandomHorizontalFlip(p=0.5),\n",
    "                     T.RandomVerticalFlip(p=0.05),\n",
    "                     T.RandomRotation(degrees=90),\n",
    "                     T.RandomGrayscale(p=0.2),\n",
    "                     T.ToDtype(torch.float32, scale=True),\n",
    "                     # T.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "                     ])\n",
    "\n",
    "script_dir = '../src/'\n",
    "aug_ox_dataset = OxPetData.OxfordPetDataset(image_dir=script_dir + 'Oxford-IIIT-Pet/images',\n",
    "                                        label_dir=script_dir + 'Oxford-IIIT-Pet/annotations/trimaps',\n",
    "                                        parameters=config.params, aug_transform=aug_transform)\n",
    "\n",
    "aug_ox_loader = DataLoader(dataset=aug_ox_dataset, batch_size=256, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mean, std \u001B[38;5;241m=\u001B[39m \u001B[43mmean_std\u001B[49m\u001B[43m(\u001B[49m\u001B[43maug_ox_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(mean)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(std)\n",
      "Cell \u001B[0;32mIn[6], line 16\u001B[0m, in \u001B[0;36mmean_std\u001B[0;34m(loader)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean_std\u001B[39m(loader):\n\u001B[1;32m     14\u001B[0m     channels_sum, channels_sqd_sum, num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 16\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchannels_sum\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchannels_sqd_sum\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw2-pt/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/_ML_UCL/_ML_MODULES/T2_MODULES/ASSIGNMENTS2/ADL_GROUP_CW/ADL_Team_Grey/augmented_data/OxfordPetDataset.py:43\u001B[0m, in \u001B[0;36mOxfordPetDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# read in image and convert to tensor\u001B[39;00m\n\u001B[1;32m     42\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m transform \u001B[38;5;241m=\u001B[39m T\u001B[38;5;241m.\u001B[39mCompose([T\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size)),\n\u001B[1;32m     44\u001B[0m                        T\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[1;32m     45\u001B[0m image_tensor \u001B[38;5;241m=\u001B[39m transform(image)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maug_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;66;03m# image_ = io.imread(image_path)\u001B[39;00m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:884\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    882\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_line:\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_suspend(thread, step_cmd)\n\u001B[0;32m--> 884\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# return event\u001B[39;00m\n\u001B[1;32m    886\u001B[0m     back \u001B[38;5;241m=\u001B[39m frame\u001B[38;5;241m.\u001B[39mf_back\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "mean, std = mean_std(aug_ox_loader)\n",
    "print(mean)\n",
    "print(std)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### APPLY THE NORMALISATION DETERMINED IN CELL ABOVE:"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aug_tfs = T.Compose([\n",
    "                        # transforms.ToPILImage(),\n",
    "                          T.ColorJitter(brightness=0.5),\n",
    "                          T.RandomHorizontalFlip(p=0.5),\n",
    "                          T.RandomVerticalFlip(p=0.05),\n",
    "                          T.RandomRotation(degrees=90),\n",
    "                          T.RandomGrayscale(p=0.2),\n",
    "                          T.ToTensor(),\n",
    "                          # T.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "                          ])\n",
    "\n",
    "script_dir = '../src/'\n",
    "aug_ox_dataset = OxPetData.OxfordPetDataset(image_dir=script_dir + 'Oxford-IIIT-Pet/images',\n",
    "                                        label_dir=script_dir + 'Oxford-IIIT-Pet/annotations/trimaps',\n",
    "                                        parameters=config.params, transform=aug_tfs)\n",
    "\n",
    "aug_ox_loader = DataLoader(dataset=aug_ox_dataset, batch_size=256, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for img_num, (img, label) in enumerate(ox_dataset):\n",
    "    if img_num == 5:\n",
    "        break\n",
    "    print(img.shape)\n",
    "    save_image(img, 'img' + str(img_num))\n",
    "    img_num += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### CUTMIX & MIXUP AUGMENTATION:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "script_dir = '../src/'\n",
    "aug_ox_dataset = OxPetData.OxfordPetDataset(image_dir=script_dir + 'Oxford-IIIT-Pet/images',\n",
    "                                        label_dir=script_dir + 'Oxford-IIIT-Pet/annotations/trimaps',\n",
    "                                        parameters=config.params, transform=aug_tfs)\n",
    "\n",
    "aug_ox_loader = DataLoader(dataset=aug_ox_dataset, batch_size=256, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "comp0197-cw2-pt",
   "language": "python",
   "display_name": "adl_cw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
