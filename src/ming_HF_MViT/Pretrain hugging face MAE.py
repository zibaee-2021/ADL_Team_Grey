# -*- coding: utf-8 -*-
"""Copy of ADL CW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dy7s-b6vS7frPfHC316orCGvHDHxOBEq
"""

# External packages
import torch
import torchvision.transforms as transforms
from torch import nn
from torch.utils.data import DataLoader
import time
import random
import numpy as np
import matplotlib.pyplot as plt

# our code
from src.utils.paths import *
from src.utils.device import get_optimal_device
from src.mvae.data_handler import (
    Animals10Dataset,
    PatchMasker,
    test_patch_masker
)
from src.shared_network_architectures.networks_pt import (
    MaskedAutoencoder,
    VisionTransformerEncoder,
    VisionTransformerDecoder
)



from transformers import ViTMAEConfig, ViTMAEModel
from transformers import AutoImageProcessor, ViTMAEForPreTraining

# Initializing a ViT MAE vit-mae-base style configuration
configuration = ViTMAEConfig(mask_ratio=0.5)
model_hf = ViTMAEForPreTraining(configuration)


from torch.utils.data import Subset


## Control
## Training
run_pretraining = True
check_masking = True
check_infilling = True
save_models = True
load_models = False

## Testing
# run_pretraining = False
# check_masking = True
# check_infilling = True
# save_models = False
# load_models = True

## Definitions
params = {
    # image
    "image_size": 224,  # number of pixels square
    "num_channels": 3,  #  RGB image -> 3 channels
    "patch_size": 16,  # must be divisor of image_size

    # vision transformer encoder
    "vit_num_features": 768,  # 768 number of features created by the vision transformer
    "vit_num_layers": 14, #12,  # 12ViT parameter
    "vit_num_heads": 8,  # 8 ViT parameter
    "vit_hidden_dim": 1024, #512,  # 512 ViT parameter
    "vit_mlp_dim": 2048, #1024,  # 1024 ViT parameter

    # vision transformer decoder
    "decoder_hidden_dim": 1024,  # 1024 ViT decoder first hidden layer dimension
    "decoder_CNN_channels": 16,  #
    "decoder_scale_factor": 4,  #

    # segmentation model
    "segmenter_hidden_dim": 128,
    "segmenter_classes": 3,  # image, background, boundary
}

report_every = 100

# Hyper-parameters
mask_ratio = 0.5
pt_batch_size = 8
pt_num_epochs = 5
pt_learning_rate = 0.00001  # 0.00001
# pt_momentum = 0.9  # not used, since using Adam optimizer
pt_step = 2
pt_gamma = 0.3

# file paths
data_dir = os.path.join(datasets_dir,"Animals-10/raw-img/")
model_file = "masked_autoencoder_model.pth"
encoder_file = "masked_autoencoder_encoder.pth"
decoder_file = "masked_autoencoder_decoder.pth"

# test image
test_image_path = os.path.join(data_dir, "image_0.png")

if __name__ == '__main__':

    # Set seeds for random number generator
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    model_path = os.path.join(models_dir, model_file)
    encoder_path = os.path.join(models_dir, encoder_file)
    decoder_path = os.path.join(models_dir, decoder_file)

    print(f"{data_dir = }\n{model_path = }\n{encoder_path = }\n{decoder_path = }\n{test_image_path = }")

    device = get_optimal_device()

    if params['image_size'] % params['patch_size'] != 0:
        raise ValueError("Alert! Patch size does not divide image size")

    num_patches = (params['image_size'] // params['patch_size']) ** 2
    num_masks = int(num_patches * mask_ratio)


    patch_masker = PatchMasker(params['patch_size'], num_masks)

    # Initializing a ViT MAE vit-mae-base style configuration
    configuration = ViTMAEConfig(mask_ratio=0.5)
    model_hf = ViTMAEForPreTraining(configuration)

    #####################
    # if a model has already been saved, load
    if load_models and os.path.isfile(model_path):
        print(f"Loading pre-saved vision-transformer model {model_file}")
        encoder.load_state_dict(torch.load(encoder_path), strict=False)
        decoder.load_state_dict(torch.load(decoder_path), strict=False)
        model_hf.load_state_dict(torch.load(model_path), strict=False)

    #####################
    # Demonstrate masking
    if check_masking:
        # original_image_tensor, masked_image_tensor = mask_tester(patch_masker, data_dir+"/cane/OIF-e2bexWrojgtQnAPPcUfOWQ.jpeg")
        original_image_tensor, masked_image_tensor = test_patch_masker(patch_masker, test_image_path)
        outputs=model_hf(original_image_tensor.unsqueeze(0))
        reconstruct=outputs.logits
        mask=outputs.mask

        # Visualize the results (assuming you have matplotlib installed)
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 4, 1)
        plt.axis('off')
        plt.imshow(original_image_tensor.squeeze(0).permute(1, 2, 0))  # Assuming image_tensor is in CHW format
        plt.title('Original Image')

        mask_=model_hf.unpatchify(mask[0].unsqueeze(-1).repeat(1, 1, model_hf.config.patch_size**2 *3))
        im_masked = original_image_tensor.squeeze(0) * (1 - mask_)
        plt.subplot(1, 4, 2)
        plt.axis('off')
        plt.imshow(im_masked.squeeze(0).permute(1, 2, 0))
        plt.title('Masked image')

        infill=model_hf.unpatchify(reconstruct)[0].detach()
        plt.subplot(1, 4, 3)
        plt.axis('off')
        plt.imshow(infill.squeeze(0).permute(1, 2, 0))  # Assuming inpainted_image_tensor is in CHW format
        plt.title('Infill Image')

        # MAE reconstruction pasted with visible patches
        im_paint = original_image_tensor * (1 - mask_) + infill * mask_
        plt.subplot(1, 4, 4)
        plt.axis('off')
        plt.imshow(im_paint.squeeze(0).permute(1, 2, 0))  # Assuming inpainted_image_tensor is in CHW format
        plt.title('Inpainted Image')

        plt.show()

    ###############
    #  mvae training
    if run_pretraining:
        print("In pre-training")
        start_time = time.perf_counter()
        model_hf.train()
        model_hf=model_hf.to(device)

        # Define loss function and optimizer
        pt_criterion = nn.MSELoss()
        pt_optimizer = torch.optim.Adam(model_hf.parameters(),lr=pt_learning_rate)
        scheduler = torch.optim.lr_scheduler.StepLR(pt_optimizer, step_size=pt_step, gamma=pt_gamma)

        # load and pre-process Animals-10 dataset and dataloader & transform to normalize the data
        transform = transforms.Compose([transforms.ToTensor()])

        dataset = Animals10Dataset(data_dir, (params['image_size'], params['image_size']), transform=transform)

        dataloader = DataLoader(dataset,
                                batch_size=pt_batch_size,
                                shuffle=True,
                                drop_last=True,
                                num_workers=2)  # drop last batch so that all batches are complete

        #use a subset of the data loader for debuging
        #subset_indices = range(100)
        #subset = Subset(dataloader.dataset, subset_indices)
        #dataloader = DataLoader(subset, batch_size=pt_batch_size, shuffle=True)

        # Main training loop
        losses = []
        for epoch in range(pt_num_epochs):
            model_hf.train()
            model_hf=model_hf.to(device)
            epoch_start_time = time.perf_counter()
            running_loss = 0.0
            for its, input_images in enumerate(dataloader):
                # print(f"[{its}, {len(dataloader)}] {time.perf_counter() - epoch_start_time:.0f}s")
                input_images = input_images.to(device)


                # Forward pass & compute the loss
                outputs = model_hf(input_images)
                infill=model_hf.unpatchify(outputs.logits)
                loss = pt_criterion(infill, input_images)

                # Backward pass and optimization
                pt_optimizer.zero_grad()
                loss.backward()
                pt_optimizer.step()

                running_loss += loss.detach().cpu().item()
                if its % report_every == (report_every - 1):  # print every report_every mini-batches
                    curr_time = time.perf_counter() - start_time
                    print('Epoch [%d / %d],  %d image minibatch [%4d / %4d], cumulative running loss: %.4f, uptime: %.2f' % (
                        epoch + 1, pt_num_epochs, pt_batch_size, its + 1, len(dataloader), running_loss / len(dataloader),
                        curr_time))
            scheduler.step()
            epoch_end_time = time.perf_counter()
            print(
                f"Epoch [{epoch + 1}/{pt_num_epochs}] completed in {(epoch_end_time - epoch_start_time):.0f}s, Loss: {running_loss / len(dataloader):.4f}")
            losses.append(running_loss / len(dataloader))

        ###############
        # Demonstrate infilling an image

            model_hf.eval()
            with torch.no_grad():
                model_hf=model_hf.to('cpu')
                outputs=model_hf(original_image_tensor.unsqueeze(0))
                reconstruct=outputs.logits
                mask=outputs.mask

            # Visualize the results (assuming you have matplotlib installed)
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 4, 1)
            plt.axis('off')
            plt.imshow(original_image_tensor.squeeze(0).permute(1, 2, 0))  # Assuming image_tensor is in CHW format
            plt.title('Original Image')

            mask_=model_hf.unpatchify(mask[0].unsqueeze(-1).repeat(1, 1, model_hf.config.patch_size**2 *3))
            im_masked = original_image_tensor.squeeze(0) * (1 - mask_)
            plt.subplot(1, 4, 2)
            plt.axis('off')
            plt.imshow(im_masked.squeeze(0).permute(1, 2, 0))
            plt.title('Masked image')

            infill=model_hf.unpatchify(reconstruct)[0].detach()
            plt.subplot(1, 4, 3)
            plt.axis('off')
            plt.imshow(infill.squeeze(0).permute(1, 2, 0))  # Assuming inpainted_image_tensor is in CHW format
            plt.title('Infill Image')

            # MAE reconstruction pasted with visible patches
            im_paint = original_image_tensor * (1 - mask_) + infill * mask_
            plt.subplot(1, 4, 4)
            plt.axis('off')
            plt.imshow(im_paint.squeeze(0).permute(1, 2, 0))  # Assuming inpainted_image_tensor is in CHW format
            plt.title('Inpainted Image')

            plt.show()

            date_str = time.strftime("_%H.%M_%d-%m-%Y", time.localtime(time.time()))
            plt.savefig(os.path.join(mvae_dir,'infilling' + date_str + '.png'))


        end_time = time.perf_counter()
        print(f"Masked VAE training finished after {(end_time - start_time):.0f}s")

        # Save the trained model & losses
        if save_models:
            torch.save(model_hf.state_dict(), model_path)


        date_str = time.strftime("_%H.%M_%d-%m-%Y", time.localtime(time.time()))
        with open(os.path.join(mvae_dir, "pt_losses" + date_str + ".txt"), 'w') as f:
            for i, loss in enumerate(losses):
                f.write(f'{i}  {loss}\n')
        print("Models saved\nFinished")
        print("Infilling finished")

    print("MVAE Script complete")